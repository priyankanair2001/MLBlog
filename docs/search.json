[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Priya Nair’s Machine Learning Blog",
    "section": "",
    "text": "Linear and Nonlinear Regression\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nPriyanka Nair\n\n\n\n\n\n\n  \n\n\n\n\nGaussian Mixture Models (GMMs)\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nPriyanka Nair\n\n\n\n\n\n\n  \n\n\n\n\nIsolation Forests\n\n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nPriyanka Nair\n\n\n\n\n\n\n  \n\n\n\n\nK-Means Clustering\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nPriyanka Nair\n\n\n\n\n\n\n  \n\n\n\n\nPrecision-Recall Curves\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nPriyanka Nair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "Hello, in this blog post, we will be discussing k-means clustering. The idea behind k-means is to group similar data points together into a predefined number of clusters. You can tell which points are more or less similar to each other depending on how far apart they are. As you may have guessed, the closer the data points are to each other, the more similar they are.\nEach data point can be thought of as a point on a graph, such that you can use the distance formula to find out how far apart two points are from one another.\n“Centroids” are the center of each cluster. It is not a data point.\nThe way this algorithm works is that initially, a number of centroids will be assigned. Then, each data point will be assigned a centroid. After that, the centroid will shift it’s position to the middle of all the data points that got assigned to it. This new centroid position can be calculated by finding the mean position of the data points that got assigned to it. Then, the data points will be reassigned to the centroid that it is now closest to. This process will keep iterating until the data points stop getting assigned to a different centroid and the centroids stop shifting.\nEven though with our own eyes we can likely distinguish different clusters in a dataset, the point of K-means clustering is for a computer to do that. The first step is to identify the number of clusters we want in our data. This is the “K” in K-means clustering.\nSometimes, even when the centroid-shifting iteration process ends, it is possible that we may end up with clusters that do not match what our eyes would tell us. We can assess the quality of the clustering by adding up the variation within each cluster. Variance is a statistical measure that quantifies the degree of spread or dispersion in a set of data points. Since K-means can’t see the clustering the way we don’t, it keeps track of the variance and starts the whole process over with different starting points.\nAfter K-means has created clusters, it will choose the cluster with the best variance.\nWhen selecting the value for K, there are different methods. One method is trial and error. After trying a few different values for K, we can identify the best one by looking at the total variations and seeing which number has the highest total variation.\nIt is important to note that even when K-means has reach its ideal values, it may not stop iterating until it’s “sure”.\n\n# Import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Create synthetic data with three clusters\nX, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n\n# Visualize the data\nplt.scatter(X[:, 0], X[:, 1], c='purple', s=30, cmap='viridis')\nplt.title(\"Synthetic Data with 3 Clusters\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n# Apply K-means clustering with k=3\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=30, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nIn this example, we can see data points that in our eyes are clearly belonging to three different clusters. However, the computer does not know that and has to go through the steps to identify the centroids shown in the bottom graph."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Precision-Recall Curves",
    "section": "",
    "text": "Hello! In this blog post, I will be teaching you about precision-recall curves. A precision-recall curve can help us measure the quality of our model. So you may be wondering… what exactly are precision and recall?\nPrecision: true positives / all positives\nRemember, all positives are true positives + false positives\nRecall: true positives / (true positives + false negatives)\nSo, you can think of precision as measuring how many selected items were relevant, and recall as how many relevant items are selected.\nPrecision and recall trade off. Increased precision leads to decreased recall and increased recall leads to decreased precision.\nNow, I will explain something really important: how do we read a graph with a precision-recall curve?\nBasically, we want both precision and recall to be as high as possible. Since precision is on the y-axis and recall is on the x-axis, the higher the points are on the graph, the more precision they indicate, and the farther right the points are, the more recall they indicate. Therefore, we want our curve to be as close to the top right corner of the graph as possible. As a result, the area under the curve should also be as high as possible.\nTo illustrate how precision-recall curves are used, I will provide an example.\nA precision-recall curve can be very useful in the real world, especially in the medical field for detecting illnesses such as breast cancer. Here, we will use sklearn’s breast cancer dataset, where a tumor will be classified as either malignant (1) or benign (0). As usual, our PR-curve will show us how good our model is.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Load the Breast Cancer dataset\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target  # 1 if malignant, 0 if benign\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Breast Cancer Classification')\nplt.legend()\nplt.show()\n\n\n\n\nAs you can see from this graph, our model seems to be quite good, as our AUC (Area Under the Curve) score is high. Here is an example of model that has a PR-Curve with a low AUC curve:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Generate a synthetic dataset with increased class imbalance\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_classes=2, n_clusters_per_class=1,\n    weights=[0.95, 0.05], flip_y=0, random_state=42\n)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression classifier\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Example Precision-Recall Curve with Increased Class Imbalance')\nplt.legend()\nplt.show()\n\n\n\n\nThis code uses a class imbalance in order to lower the AUC score. It basically is data that can represent something to be classified and something that does not fall under that classification(i.e.: spam vs not spam). Class imbalance refers to a situation in a classification problem where the distribution of examples across different classes is not equal. In other words, one class has significantly more instances than the other class or classes. Class imbalance is common in many real-world datasets.\nFor example, consider a binary classification problem where you want to predict whether an email is spam or not. If 95% of the emails are not spam and only 5% are spam, the dataset is said to have class imbalance.\nIn some cases, a higher class imbalance might lead to a higher AUC-PR, especially if the model is able to correctly identify the minority class instances (high recall) while maintaining high precision. In such cases, the model can effectively separate the minority class from the majority class, resulting in a better AUC-PR.\nHowever, in highly imbalanced scenarios, achieving both high precision and high recall can be challenging. The model might be biased towards the majority class, leading to high precision for the majority class but potentially low recall for the minority class."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Priyanka Nair, but you can call me Priya. I graduated from Virginia Tech with a Bachelor’s in Computer Science in May 2023. I am currently working towards my Master’s in Computer Science. This blog is to showcase what I’ve learned in my Machine Learning class. My favorite color is (obviously) purple."
  },
  {
    "objectID": "posts/Clustering copy/index.html",
    "href": "posts/Clustering copy/index.html",
    "title": "K-Means Clustering COPY",
    "section": "",
    "text": "Hello, in this blog post, we will be discussing k-means clustering. The idea behind k-means is to group similar data points together into a predefined number of clusters. You can tell which points are more or less similar to each other depending on how far apart they are. As you may have guessed, the closer the data points are to each other, the more similar they are.\nEach data point can be thought of as a point on a graph, such that you can use the distance formula to find out how far apart two points are from one another.\n“Centroids” are the center of each cluster. It is not a data point.\nThe way this algorithm works is that initially, a number of centroids will be assigned. Then, each data point will be assigned a centroid. After that, the centroid will shift it’s position to the middle of all the data points that got assigned to it. This new centroid position can be calculated by finding the mean position of the data points that got assigned to it. Then, the data points will be reassigned to the centroid that it is now closest to. This process will keep iterating until the data points stop getting assigned to a different centroid and the centroids stop shifting.\nEven though with our own eyes we can likely distinguish different clusters in a dataset, the point of K-means clustering is for a computer to do that. The first step is to identify the number of clusters we want in our data. This is the “K” in K-means clustering.\nSometimes, even when the centroid-shifting iteration process ends, it is possible that we may end up with clusters that do not match what our eyes would tell us. We can assess the quality of the clustering by adding up the variation within each cluster. Variance is a statistical measure that quantifies the degree of spread or dispersion in a set of data points. Since K-means can’t see the clustering the way we don’t, it keeps track of the variance and starts the whole process over with different starting points.\nAfter K-means has created clusters, it will choose the cluster with the best variance.\nWhen selecting the value for K, there are different methods. One method is trial and error. After trying a few different values for K, we can identify the best one by looking at the total variations and seeing which number has the highest total variation.\nIt is important to note that even when K-means has reach its ideal values, it may not stop iterating until it’s “sure”.\n\n# Import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Create synthetic data with three clusters\nX, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n\n# Visualize the data\nplt.scatter(X[:, 0], X[:, 1], c='purple', s=30, cmap='viridis')\nplt.title(\"Synthetic Data with 3 Clusters\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n# Apply K-means clustering with k=3\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=30, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nIn this example, we can see data points that in our eyes are clearly belonging to three different clusters. However, the computer does not know that and has to go through the steps to identify the centroids shown in the bottom graph."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Precision-Recall Curves",
    "section": "",
    "text": "Hello! In this blog post, I will be teaching you about precision-recall curves. A precision-recall curve can help us measure the quality of our model. So you may be wondering… what exactly are precision and recall?\nPrecision: true positives / all positives\nRemember, all positives are true positives + false positives\nRecall: true positives / (true positives + false negatives)\nSo, you can think of precision as measuring how many selected items were relevant, and recall as how many relevant items are selected.\nPrecision and recall trade off. Increased precision leads to decreased recall and increased recall leads to decreased precision.\nNow, I will explain something really important: how do we read a graph with a precision-recall curve?\nBasically, we want both precision and recall to be as high as possible. Since precision is on the y-axis and recall is on the x-axis, the higher the points are on the graph, the more precision they indicate, and the farther right the points are, the more recall they indicate. Therefore, we want our curve to be as close to the top right corner of the graph as possible. As a result, the area under the curve should also be as high as possible.\nTo illustrate how precision-recall curves are used, I will provide an example.\nA precision-recall curve can be very useful in the real world, especially in the medical field for detecting illnesses such as breast cancer. Here, we will use sklearn’s breast cancer dataset, where a tumor will be classified as either malignant (1) or benign (0). As usual, our PR-curve will show us how good our model is.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Load the Breast Cancer dataset\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target  # 1 if malignant, 0 if benign\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Breast Cancer Classification')\nplt.legend()\nplt.show()\n\n\n\n\nAs you can see from this graph, our model seems to be quite good, as our AUC (Area Under the Curve) score is high. Here is an example of model that has a PR-Curve with a low AUC curve:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Generate a synthetic dataset with increased class imbalance\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_classes=2, n_clusters_per_class=1,\n    weights=[0.95, 0.05], flip_y=0, random_state=42\n)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression classifier\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Example Precision-Recall Curve with Increased Class Imbalance')\nplt.legend()\nplt.show()\n\n\n\n\nThis code uses a class imbalance in order to lower the AUC score. It basically is data that can represent something to be classified and something that does not fall under that classification(i.e.: spam vs not spam). Class imbalance refers to a situation in a classification problem where the distribution of examples across different classes is not equal. In other words, one class has significantly more instances than the other class or classes. Class imbalance is common in many real-world datasets.\nFor example, consider a binary classification problem where you want to predict whether an email is spam or not. If 95% of the emails are not spam and only 5% are spam, the dataset is said to have class imbalance.\nIn some cases, a higher class imbalance might lead to a higher AUC-PR, especially if the model is able to correctly identify the minority class instances (high recall) while maintaining high precision. In such cases, the model can effectively separate the minority class from the majority class, resulting in a better AUC-PR.\nHowever, in highly imbalanced scenarios, achieving both high precision and high recall can be challenging. The model might be biased towards the majority class, leading to high precision for the majority class but potentially low recall for the minority class.\nThank you to ChatGPT for the code and YouTube tutorial https://www.youtube.com/watch?v=-zwanhruRHE for teaching me."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "Hello, in this blog post, we will be discussing k-means clustering. The idea behind k-means is to group similar data points together into a predefined number of clusters. You can tell which points are more or less similar to each other depending on how far apart they are. As you may have guessed, the closer the data points are to each other, the more similar they are.\nEach data point can be thought of as a point on a graph, such that you can use the distance formula to find out how far apart two points are from one another.\n“Centroids” are the center of each cluster. It is not a data point.\nThe way this algorithm works is that initially, a number of centroids will be assigned. Then, each data point will be assigned a centroid. After that, the centroid will shift it’s position to the middle of all the data points that got assigned to it. This new centroid position can be calculated by finding the mean position of the data points that got assigned to it. Then, the data points will be reassigned to the centroid that it is now closest to. This process will keep iterating until the data points stop getting assigned to a different centroid and the centroids stop shifting.\nEven though with our own eyes we can likely distinguish different clusters in a dataset, the point of K-means clustering is for a computer to do that. The first step is to identify the number of clusters we want in our data. This is the “K” in K-means clustering.\nSometimes, even when the centroid-shifting iteration process ends, it is possible that we may end up with clusters that do not match what our eyes would tell us. We can assess the quality of the clustering by adding up the variation within each cluster. Variance is a statistical measure that quantifies the degree of spread or dispersion in a set of data points. Since K-means can’t see the clustering the way we don’t, it keeps track of the variance and starts the whole process over with different starting points.\nAfter K-means has created clusters, it will choose the cluster with the best variance.\nWhen selecting the value for K, there are different methods. One method is trial and error. After trying a few different values for K, we can identify the best one by looking at the total variations and seeing which number has the highest total variation.\nIt is important to note that even when K-means has reach its ideal values, it may not stop iterating until it’s “sure”.\n\n# Import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Create synthetic data with three clusters\nX, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n\n# Visualize the data\nplt.scatter(X[:, 0], X[:, 1], c='purple', s=30, cmap='viridis')\nplt.title(\"Synthetic Data with 3 Clusters\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n# Apply K-means clustering with k=3\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=30, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nIn this example, we can see data points that in our eyes are clearly belonging to three different clusters. However, the computer does not know that and has to go through the steps to identify the centroids shown in the bottom graph.\nThank you to ChatGPT for the code and YouTube tutorials https://www.youtube.com/watch?v=4b5d3muPQmA and https://www.youtube.com/watch?v=R2e3Ls9H_fc for teaching me about this topic!"
  },
  {
    "objectID": "posts/Anomaly Detection/index.html",
    "href": "posts/Anomaly Detection/index.html",
    "title": "Isolation Forests",
    "section": "",
    "text": "Hi everyone! In this post we will be going over anomaly and outlier detection. We will be going over how we can use isolation forests for anomaly detection.\nSo first, let’s go over what anomaly detection is. Anomaly detection is when you have a group of points and you want to find the points that do not align or belong to the rest of the points. These points will stand out from the rest and be outliers. A real world example of using anomaly detection would be when you want to find fraudulant credit card transactions.\nNow, let’s go over what isolation forests are. They are a variation of random forests that can be used specifically for anomaly detection. To recap what random forests are, they are decision trees. Decision trees separate the data one at a time until each part in homogeneous. They can be used for classification purposes. Isolation trees also separate data, but not into groups. They instead isolate each data point. The way isolation forests can work to detect anomalies is that when they separate each point, the points that take the least amount of separations are more likely to be anomalies. This is because when there are other points nearby a point, there is a high chance that points will be in the same partitioned areas as other points, unless you keep partitioning an area such that it becomes only large enough to fit one point.\nIn the algorithm, isolation forests build a forest of isolation trees. What happens is that for each point we have to build an isolation tree until each point is separated.\nWhen we predict the anomalies, we have a formula to give each point an “anomaly score”.\n\nVariables:\nx: new point\nm: sample size\ns(x,m) : anomaly score\nh(x): average search height of x from isolation trees\nc(m): average value of h(x)\nE(h(x)) : average path length of multiple isolation trees\nThe smaller the anomaly score, the more likely the instance is considered an anomaly.\nIn the example of code shown below, we have a dataset with regular data and outliers as can be seen in the first graph. Our isolation forests algorithm will try to detect the the anomalies:\n\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\n\n# Generate synthetic data with outliers\nnp.random.seed(42)\nnormal_data = np.random.normal(loc=0, scale=1, size=(1000, 2))\noutliers = np.random.uniform(low=-10, high=10, size=(50, 2))\ndata = np.vstack([normal_data, outliers])\n\n# Visualize the data\nplt.scatter(data[:, 0], data[:, 1], s=5, label='Normal Data')\nplt.scatter(outliers[:, 0], outliers[:, 1], s=20, color='red', label='Outliers')\nplt.title('Synthetic Data with Outliers')\nplt.legend()\nplt.show()\n\n# Apply Isolation Forest for anomaly detection\nclf = IsolationForest(contamination=0.05)  # Adjust the contamination parameter based on the expected outlier proportion\nclf.fit(data)\n\n# Predict outliers\npredictions = clf.predict(data)\n\n# Visualize the results\nplt.scatter(data[:, 0], data[:, 1], s=5, c=predictions, cmap='viridis')\nplt.title('Isolation Forest Anomaly Detection Results')\nplt.show()\n\n\n\n\n\n\n\nOur algorithm managed to mostly predict the points that were clearly outliers. However, it had a little bit of trouble with points that close to or in the center cluster.\nThank you to ChatGPT for the code and YouTube tutorial https://www.youtube.com/watch?v=cRzeotaFDwk for teaching me."
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Hello, and welcome to my latest blog post on Machine Learning. Today, we will be talking about linear and non-linear regression. First we will be going over linear regression.\nIt’s important for machine learning algorithms to be able to predict a certain metric given that it is correlated with another metric. For example, given a person’s height, a machine learning model may be able to (roughly) predict their weight.\nFinding the right function to prediction one variable (y) from another (x) is known as regression. It’s easiest to assume that the function is linear, hence where we get “linear regression”.\nWe’ve all learned about the formula y = mx+b. The goal of linear regression is to find the m and b values that will create a line that will fit the data the best.\n\nIn order to assess the “fit” of the line, we can take the difference of where a point is and where it should be according to the line, and square that value and take the sum of this value for all data points.\n\nTo find out m and b in the y = mx+b formula from here, we can take the gradient of the sum value equation discussed in the previous paragraph, set it to zero, and then solve for m and b (also sometimes known as alpha and beta).\n\n\nObviously, we do not have to do all of this math by hand, as we can simply use Python and its libraries to do the work for us. All we have to do (as seen in the code example below) is import scikit learn, declare a linear regression model, feed it the training data, and call the fit function.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate some example data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n# Plot the results\nplt.scatter(X_test, y_test, color='black')\nplt.plot(X_test, y_pred, color='purple', linewidth=3)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression Example')\nplt.show()\n\nMean Squared Error: 0.6536995137170021\n\n\n\n\n\nNow we will move on to non-linear regression. As you may have guessed based on it’s name, non-linear regression is similar to linear regression except instead of producing a line of best fit for the data points, it produces a curve of a different shape that accurately matches the trajectory of the data points.\nThere are different types of non-linear regression including polynomial regression, which consists of quadratic and cubic regressions. Exponential and logarithmic regressions exist as well.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate some example data with a non-linear relationship\nnp.random.seed(42)\nX = 6 * np.random.rand(100, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply polynomial features\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the polynomial features\nmodel.fit(X_poly_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_poly_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n# Plot the results\nplt.scatter(X_test, y_test, color='black')\nplt.scatter(X_test, y_pred, color='purple', marker='x')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Polynomial Regression Example')\nplt.show()\n\nMean Squared Error: 0.6358406072820808\n\n\n\n\n\nThank you to ChatGPT for the code and YouTube tutorials https://www.youtube.com/watch?v=CtsRRUddV2s and https://www.youtube.com/watch?v=av4zxt2bV6A for teaching me."
  },
  {
    "objectID": "posts/Probability Theory and Random Variables/index.html",
    "href": "posts/Probability Theory and Random Variables/index.html",
    "title": "Gaussian Mixture Models (GMMs)",
    "section": "",
    "text": "Hi! In this post, we will discuss about the applications of probability theory and random variables in Machine Learning. For our example, we will be exploring how the Gaussian Mixtures clustering algorithm can generate n x k cluster label probabilities.\nTo start off, I will be teaching you about Gaussian Mixture Models(GMMs). In a previous post, I taught you about the K-Means clustering algorithm. GMMs are also used for clustering. However, a key difference between these two is that K-means assigns it’s data points to the cluster of its nearest neighbors, while GMMs use a probabilistic approach of clustering in the sense that it will tell you the probability of a certain data point belonging to a certain cluster.\nWhen you want to use a GMM, if you have a single cluster, it is easy to simply put a GMM on top of this clusters. However, when we have multiple clusters, we need more GMMs (one for each cluster).\nTo find the best possible fit for our Gaussians, we can use something called the expectation-maximization algorithm. It helps estimate parameters in statistical models where data is incomplete. It consists of two steps:\nThe Expectation Step helps it calculate the value of the function given the current parameter estimates\nThe Maximization Step updates the parameters estimates to maximize what was calculated in the expectation step\nWhen we alternate between these two steps, we will be able to refine our parameter estimates.\nInitially, this will give us an idea of where each Gaussian should be centered, how their covariance should look, and how much weight should be provided to each Gaussian in the final distribution.\nThe overall GMM probability is equivalent to the sum of all Gaussians for which we want to add up the weight of each Gaussian and multiple it by the probability of that Gaussian.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data with three clusters\nX, y = make_blobs(n_samples=300, centers=3, random_state=42)\n\n# Fit a Gaussian Mixture Model\ngmm = GaussianMixture(n_components=3)\ngmm.fit(X)\n\n# Predict cluster labels and probabilities\nlabels = gmm.predict(X)\nprobs = gmm.predict_proba(X)\n\n# Plot the data points with colors representing predicted clusters\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, edgecolors='k')\n\n# Plot the cluster centers\nplt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], marker='x', s=200, linewidths=3, color='r', label='Cluster Centers')\n\nplt.title('Gaussian Mixture Model')\nplt.legend()\nplt.show()\n\n# Display cluster label probabilities for the first few data points\nprint(\"Cluster Label Probabilities:\")\nprint(probs[:5])\n\n\n\n\nCluster Label Probabilities:\n[[1.35999675e-60 1.00000000e+00 5.79506157e-67]\n [3.14154567e-60 1.00000000e+00 1.36438005e-64]\n [1.72369478e-15 2.05865579e-51 1.00000000e+00]\n [1.00000000e+00 1.71564500e-45 8.23076192e-24]\n [9.85406873e-70 1.00000000e+00 1.20447754e-71]]\n\n\nThis is an example of a GMM and its cluster label probabilities.\nThank you to ChatGPT for the code and YouTube tutorial https://www.youtube.com/watch?v=wT2yLNUfyoM for teaching me about this topic!"
  }
]