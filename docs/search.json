[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Priya Nair’s Machine Learning Blog",
    "section": "",
    "text": "K-Means Clustering\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nPriyanka Nair\n\n\n\n\n\n\n  \n\n\n\n\nPrecision-Recall Curves\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nPriyanka Nair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "Hello, in this blog post, we will be discussing k-means clustering. The idea behind k-means is to group similar data points together into a predefined number of clusters. You can tell which points are more or less similar to each other depending on how far apart they are. As you may have guessed, the closer the data points are to each other, the more similar they are.\nEach data point can be thought of as a point on a graph, such that you can use the distance formula to find out how far apart two points are from one another.\n“Centroids” are the center of each cluster. It is not a data point.\nThe way this algorithm works is that initially, a number of centroids will be assigned. Then, each data point will be assigned a centroid. After that, the centroid will shift it’s position to the middle of all the data points that got assigned to it. This new centroid position can be calculated by finding the mean position of the data points that got assigned to it. Then, the data points will be reassigned to the centroid that it is now closest to. This process will keep iterating until the data points stop getting assigned to a different centroid and the centroids stop shifting.\nEven though with our own eyes we can likely distinguish different clusters in a dataset, the point of K-means clustering is for a computer to do that. The first step is to identify the number of clusters we want in our data. This is the “K” in K-means clustering.\nSometimes, even when the centroid-shifting iteration process ends, it is possible that we may end up with clusters that do not match what our eyes would tell us. We can assess the quality of the clustering by adding up the variation within each cluster. Variance is a statistical measure that quantifies the degree of spread or dispersion in a set of data points. Since K-means can’t see the clustering the way we don’t, it keeps track of the variance and starts the whole process over with different starting points.\nAfter K-means has created clusters, it will choose the cluster with the best variance.\nWhen selecting the value for K, there are different methods. One method is trial and error. After trying a few different values for K, we can identify the best one by looking at the total variations and seeing which number has the highest total variation.\nIt is important to note that even when K-means has reach its ideal values, it may not stop iterating until it’s “sure”.\n\n# Import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\n# Create synthetic data with three clusters\nX, y = make_blobs(n_samples=300, centers=3, random_state=42, cluster_std=1.0)\n\n# Visualize the data\nplt.scatter(X[:, 0], X[:, 1], c='purple', s=30, cmap='viridis')\nplt.title(\"Synthetic Data with 3 Clusters\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n# Apply K-means clustering with k=3\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\n\n# Get cluster centers and labels\ncenters = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\n# Visualize the clustered data\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=30, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nIn this example, we can see data points that in our eyes are clearly belonging to three different clusters. However, the computer does not know that and has to go through the steps to identify the centroids shown in the bottom graph."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Precision-Recall Curves",
    "section": "",
    "text": "Hello! In this blog post, I will be teaching you about precision-recall curves. A precision-recall curve can help us measure the quality of our model. So you may be wondering… what exactly are precision and recall?\nPrecision: true positives / all positives\nRemember, all positives are true positives + false positives\nRecall: true positives / (true positives + false negatives)\nSo, you can think of precision as measuring how many selected items were relevant, and recall as how many relevant items are selected.\nPrecision and recall trade off. Increased precision leads to decreased recall and increased recall leads to decreased precision.\nNow, I will explain something really important: how do we read a graph with a precision-recall curve?\nBasically, we want both precision and recall to be as high as possible. Since precision is on the y-axis and recall is on the x-axis, the higher the points are on the graph, the more precision they indicate, and the farther right the points are, the more recall they indicate. Therefore, we want our curve to be as close to the top right corner of the graph as possible. As a result, the area under the curve should also be as high as possible.\nTo illustrate how precision-recall curves are used, I will provide an example.\nA precision-recall curve can be very useful in the real world, especially in the medical field for detecting illnesses such as breast cancer. Here, we will use sklearn’s breast cancer dataset, where a tumor will be classified as either malignant (1) or benign (0). As usual, our PR-curve will show us how good our model is.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Load the Breast Cancer dataset\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target  # 1 if malignant, 0 if benign\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Breast Cancer Classification')\nplt.legend()\nplt.show()\n\n\n\n\nAs you can see from this graph, our model seems to be quite good, as our AUC (Area Under the Curve) score is high. Here is an example of model that has a PR-Curve with a low AUC curve:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Generate a synthetic dataset with increased class imbalance\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_classes=2, n_clusters_per_class=1,\n    weights=[0.95, 0.05], flip_y=0, random_state=42\n)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression classifier\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Example Precision-Recall Curve with Increased Class Imbalance')\nplt.legend()\nplt.show()\n\n\n\n\nThis code uses a class imbalance in order to lower the AUC score. It basically is data that can represent something to be classified and something that does not fall under that classification(i.e.: spam vs not spam). Class imbalance refers to a situation in a classification problem where the distribution of examples across different classes is not equal. In other words, one class has significantly more instances than the other class or classes. Class imbalance is common in many real-world datasets.\nFor example, consider a binary classification problem where you want to predict whether an email is spam or not. If 95% of the emails are not spam and only 5% are spam, the dataset is said to have class imbalance.\nIn some cases, a higher class imbalance might lead to a higher AUC-PR, especially if the model is able to correctly identify the minority class instances (high recall) while maintaining high precision. In such cases, the model can effectively separate the minority class from the majority class, resulting in a better AUC-PR.\nHowever, in highly imbalanced scenarios, achieving both high precision and high recall can be challenging. The model might be biased towards the majority class, leading to high precision for the majority class but potentially low recall for the minority class."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, my name is Priyanka Nair, but you can call me Priya. I graduated from Virginia Tech with a Bachelor’s in Computer Science in May 2023. I am currently working towards my Master’s in Computer Science. This blog is to showcase what I’ve learned in my Machine Learning class. My favorite color is (obviously) purple."
  }
]