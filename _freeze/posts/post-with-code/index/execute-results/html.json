{
  "hash": "0b71281b456b46097f6d861d2307da26",
  "result": {
    "markdown": "---\ntitle: \"Precision-Recall Curves\"\nauthor: \"Priyanka Nair\"\ndate: \"2023-11-13\"\ncategories: [Classification]\n---\n\nHello! In this blog post, I will be teaching you about precision-recall curves. A precision-recall curve can help us measure the quality of our model. So you may be wondering... what exactly are precision and recall?\n\nPrecision: true positives / all positives\n\nRemember, all positives are true positives + false positives\n\nRecall: true positives / (true positives + false negatives)\n\nSo, you can think of precision as measuring how many selected items were relevant, and recall as how many relevant items are selected.\n\nPrecision and recall trade off. Increased precision leads to decreased recall and increased recall leads to decreased precision.\n\nNow, I will explain something really important: how do we read a graph with a precision-recall curve?\n\nBasically, we want both precision and recall to be as high as possible. Since precision is on the y-axis and recall is on the x-axis, the higher the points are on the graph, the more precision they indicate, and the farther right the points are, the more recall they indicate. Therefore, we want our curve to be as close to the top right corner of the graph as possible. As a result, the area under the curve should also be as high as possible.\n\nTo illustrate how precision-recall curves are used, I will provide an example.\n\nA precision-recall curve can be very useful in the real world, especially in the medical field for detecting illnesses such as breast cancer. Here, we will use sklearn's breast cancer dataset, where a tumor will be classified as either malignant (1) or benign (0). As usual, our PR-curve will show us how good our model is.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Load the Breast Cancer dataset\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target  # 1 if malignant, 0 if benign\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve for Breast Cancer Classification')\nplt.legend()\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=672 height=523}\n:::\n:::\n\n\nAs you can see from this graph, our model seems to be quite good, as our AUC (Area Under the Curve) score is high. Here is an example of model that has a PR-Curve with a low AUC curve:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\n\n# Generate a synthetic dataset with increased class imbalance\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_classes=2, n_clusters_per_class=1,\n    weights=[0.95, 0.05], flip_y=0, random_state=42\n)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression classifier\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_scores = model.predict_proba(X_test)[:, 1]\n\n# Calculate precision and recall\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n\n# Calculate the area under the precision-recall curve\nauc_score = auc(recall, precision)\n\n# Plot the precision-recall curve\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='purple', label=f'PR Curve (AUC = {auc_score:.2f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Example Precision-Recall Curve with Increased Class Imbalance')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=663 height=523}\n:::\n:::\n\n\nThis code uses a class imbalance in order to lower the AUC score. It basically is data that can represent something to be classified and something that does not fall under that classification(i.e.: spam vs not spam). Class imbalance refers to a situation in a classification problem where the distribution of examples across different classes is not equal. In other words, one class has significantly more instances than the other class or classes. Class imbalance is common in many real-world datasets.\n\nFor example, consider a binary classification problem where you want to predict whether an email is spam or not. If 95% of the emails are not spam and only 5% are spam, the dataset is said to have class imbalance.\n\nIn some cases, a higher class imbalance might lead to a higher AUC-PR, especially if the model is able to correctly identify the minority class instances (high recall) while maintaining high precision. In such cases, the model can effectively separate the minority class from the majority class, resulting in a better AUC-PR.\n\nHowever, in highly imbalanced scenarios, achieving both high precision and high recall can be challenging. The model might be biased towards the majority class, leading to high precision for the majority class but potentially low recall for the minority class.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}