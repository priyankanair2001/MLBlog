{
  "hash": "b79d52676af93892b7117d34d7bafc11",
  "result": {
    "markdown": "---\ntitle: \"Gaussian Mixture Models (GMMs)\"\nauthor: \"Priyanka Nair\"\ndate: \"2023-12-06\"\ncategories: [Probability Theory and Random Variables]\n---\n\nHi! In this post, we will discuss about the applications of probability theory and random variables in Machine Learning. For our example, we will be exploring how the Gaussian Mixtures clustering algorithm can generate n x k cluster label probabilities.\n\nTo start off, I will be teaching you about Gaussian Mixture Models(GMMs). In a previous post, I taught you about the K-Means clustering algorithm. GMMs are also used for clustering. However, a key difference between these two is that K-means assigns it's data points to the cluster of its nearest neighbors, while GMMs use a probabilistic approach of clustering in the sense that it will tell you the probability of a certain data point belonging to a certain cluster.\n\nWhen you want to use a GMM, if you have a single cluster, it is easy to simply put a GMM on top of this clusters. However, when we have multiple clusters, we need more GMMs (one for each cluster).\n\nTo find the best possible fit for our Gaussians, we can use something called the expectation-maximization algorithm. It helps estimate parameters in statistical models where data is incomplete. It consists of two steps:\n\nThe Expectation Step helps it calculate the value of the function given the current parameter estimates\n\nThe Maximization Step updates the parameters estimates to maximize what was calculated in the expectation step\n\nWhen we alternate between these two steps, we will be able to refine our parameter estimates.\n\nInitially, this will give us an idea of where each Gaussian should be centered, how their covariance should look, and how much weight should be provided to each Gaussian in the final distribution.\n\nThe overall GMM probability is equivalent to the sum of all Gaussians for which we want to add up the weight of each Gaussian and multiple it by the probability of that Gaussian.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data with three clusters\nX, y = make_blobs(n_samples=300, centers=3, random_state=42)\n\n# Fit a Gaussian Mixture Model\ngmm = GaussianMixture(n_components=3)\ngmm.fit(X)\n\n# Predict cluster labels and probabilities\nlabels = gmm.predict(X)\nprobs = gmm.predict_proba(X)\n\n# Plot the data points with colors representing predicted clusters\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, edgecolors='k')\n\n# Plot the cluster centers\nplt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], marker='x', s=200, linewidths=3, color='r', label='Cluster Centers')\n\nplt.title('Gaussian Mixture Model')\nplt.legend()\nplt.show()\n\n# Display cluster label probabilities for the first few data points\nprint(\"Cluster Label Probabilities:\")\nprint(probs[:5])\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=577 height=431}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster Label Probabilities:\n[[1.35999675e-60 1.00000000e+00 5.79506157e-67]\n [3.14154567e-60 1.00000000e+00 1.36438005e-64]\n [1.72369478e-15 2.05865579e-51 1.00000000e+00]\n [1.00000000e+00 1.71564500e-45 8.23076192e-24]\n [9.85406873e-70 1.00000000e+00 1.20447754e-71]]\n```\n:::\n:::\n\n\nThis is an example of a GMM and its cluster label probabilities.\n\nThank you to ChatGPT for the code and YouTube tutorial https://www.youtube.com/watch?v=wT2yLNUfyoM for teaching me about this topic!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}