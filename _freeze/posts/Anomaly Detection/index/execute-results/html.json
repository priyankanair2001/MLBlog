{
  "hash": "2326f16a738249eab69cf7d944a41544",
  "result": {
    "markdown": "---\ntitle: \"Isolation Forests\"\nauthor: \"Priyanka Nair\"\ndate: \"2023-12-04\"\ncategories: [Anomaly/Outlier Detection]\n---\n\nHi everyone! In this post we will be going over anomaly and outlier detection. We will be going over how we can use isolation forests for anomaly detection.\n\nSo first, let's go over what anomaly detection is. Anomaly detection is when you have a group of points and you want to find the points that do not align or belong to the rest of the points. These points will stand out from the rest and be outliers. A real world example of using anomaly detection would be when you want to find fraudulant credit card transactions.\n\nNow, let's go over what isolation forests are. They are a variation of random forests that can be used specifically for anomaly detection. To recap what random forests are, they are decision trees. Decision trees separate the data one at a time until each part in homogeneous. They can be used for classification purposes. Isolation trees also separate data, but not into groups. They instead isolate each data point. The way isolation forests can work to detect anomalies is that when they separate each point, the points that take the least amount of separations are more likely to be anomalies. This is because when there are other points nearby a point, there is a high chance that points will be in the same partitioned areas as other points, unless you keep partitioning an area such that it becomes only large enough to fit one point.\n\nIn the algorithm, isolation forests build a forest of isolation trees. What happens is that for each point we have to build an isolation tree until each point is separated.\n\nWhen we predict the anomalies, we have a formula to give each point an \"anomaly score\".\n\n![](images/Screenshot 2023-12-06 at 11.17.04 AM.png){width=\"226\"}\n\nVariables:\n\nx: new point\n\nm: sample size\n\ns(x,m) : anomaly score\n\nh(x): average search height of x from isolation trees\n\nc(m): average value of h(x)\n\nE(h(x)) : average path length of multiple isolation trees\n\nThe smaller the anomaly score, the more likely the instance is considered an anomaly.\n\nIn the example of code shown below, we have a dataset with regular data and outliers as can be seen in the first graph. Our isolation forests algorithm will try to detect the the anomalies:\\\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\n\n# Generate synthetic data with outliers\nnp.random.seed(42)\nnormal_data = np.random.normal(loc=0, scale=1, size=(1000, 2))\noutliers = np.random.uniform(low=-10, high=10, size=(50, 2))\ndata = np.vstack([normal_data, outliers])\n\n# Visualize the data\nplt.scatter(data[:, 0], data[:, 1], s=5, label='Normal Data')\nplt.scatter(outliers[:, 0], outliers[:, 1], s=20, color='red', label='Outliers')\nplt.title('Synthetic Data with Outliers')\nplt.legend()\nplt.show()\n\n# Apply Isolation Forest for anomaly detection\nclf = IsolationForest(contamination=0.05)  # Adjust the contamination parameter based on the expected outlier proportion\nclf.fit(data)\n\n# Predict outliers\npredictions = clf.predict(data)\n\n# Visualize the results\nplt.scatter(data[:, 0], data[:, 1], s=5, c=predictions, cmap='viridis')\nplt.title('Isolation Forest Anomaly Detection Results')\nplt.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=591 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=591 height=431}\n:::\n:::\n\n\nOur algorithm managed to mostly predict the points that were clearly outliers. However, it had a little bit of trouble with points that close to or in the center cluster.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}