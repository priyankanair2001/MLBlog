{
  "hash": "4fa68dfb625528483e44a63507a3cb99",
  "result": {
    "markdown": "---\ntitle: \"Linear and Nonlinear Regression\"\nauthor: \"Priyanka Nair\"\ndate: \"2023-12-06\"\ncategories: [Linear and Nonlinear Regression]\n---\n\nHello, and welcome to my latest blog post on Machine Learning. Today, we will be talking about linear and non-linear regression. First we will be going over linear regression.\n\nIt's important for machine learning algorithms to be able to predict a certain metric given that it is correlated with another metric. For example, given a person's height, a machine learning model may be able to (roughly) predict their weight.\n\nFinding the right function to prediction one variable (y) from another (x) is known as regression. It's easiest to assume that the function is linear, hence where we get \"linear regression\".\n\nWe've all learned about the formula y = mx+b. The goal of linear regression is to find the m and b values that will create a line that will fit the data the best.\n\n![](images/Screenshot 2023-12-06 at 3.59.13 PM.png){width=\"198\"}\n\nIn order to assess the \"fit\" of the line, we can take the difference of where a point is and where it should be according to the line, and square that value and take the sum of this value for all data points.\n\n![](images/Screenshot 2023-12-06 at 4.00.18 PM.png){width=\"198\"}\n\nTo find out m and b in the y = mx+b formula from here, we can take the gradient of the sum value equation discussed in the previous paragraph, set it to zero, and then solve for m and b (also sometimes known as alpha and beta).\n\n![](images/Screenshot 2023-12-06 at 7.12.10 PM.png){width=\"221\"}\n\n![](images/Screenshot 2023-12-06 at 7.13.14 PM.png){width=\"222\"}\n\nObviously, we do not have to do all of this math by hand, as we can simply use Python and its libraries to do the work for us. All we have to do (as seen in the code example below) is import scikit learn, declare a linear regression model, feed it the training data, and call the fit function.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate some example data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n# Plot the results\nplt.scatter(X_test, y_test, color='black')\nplt.plot(X_test, y_pred, color='purple', linewidth=3)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression Example')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.6536995137170021\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n:::\n\n\nNow we will move on to non-linear regression. As you may have guessed based on it's name, non-linear regression is similar to linear regression except instead of producing a line of best fit for the data points, it produces a curve of a different shape that accurately matches the trajectory of the data points.\n\nThere are different types of non-linear regression including polynomial regression, which consists of quadratic and cubic regressions. Exponential and logarithmic regressions exist as well.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generate some example data with a non-linear relationship\nnp.random.seed(42)\nX = 6 * np.random.rand(100, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Apply polynomial features\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly_train = poly_features.fit_transform(X_train)\nX_poly_test = poly_features.transform(X_test)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the polynomial features\nmodel.fit(X_poly_train, y_train)\n\n# Make predictions on the testing set\ny_pred = model.predict(X_poly_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\n\n# Plot the results\nplt.scatter(X_test, y_test, color='black')\nplt.scatter(X_test, y_pred, color='purple', marker='x')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Polynomial Regression Example')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.6358406072820808\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=576 height=449}\n:::\n:::\n\n\nThank you to ChatGPT for the code and YouTube tutorials https://www.youtube.com/watch?v=CtsRRUddV2s and https://www.youtube.com/watch?v=av4zxt2bV6A for teaching me.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}